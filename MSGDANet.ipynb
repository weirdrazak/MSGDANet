{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtGN02CJbkn77e8fn0n3Va",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weirdrazak/MSGDANet/blob/main/MSGDANet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wymKV66jyXGp",
        "outputId": "f7d04a0d-4cd0-46e4-99a1-9056a979cd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adan_pytorch\n",
            "  Downloading adan_pytorch-0.1.0-py3-none-any.whl.metadata (661 bytes)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from adan_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (2025.7.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->adan_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->adan_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->adan_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->adan_pytorch) (3.0.2)\n",
            "Downloading adan_pytorch-0.1.0-py3-none-any.whl (3.4 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install adan_pytorch albumentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "import torch\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import warnings  # To suppress specific sklearn warnings\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    cohen_kappa_score,\n",
        "    precision_recall_curve,\n",
        "    auc\n",
        ")\n",
        "from tabulate import tabulate  # For pretty printing metrics table\n",
        "from torch.optim.lr_scheduler import PolynomialLR\n",
        "from tqdm import tqdm\n",
        "from adan_pytorch import Adan\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from collections import defaultdict\n",
        "\n",
        "# VGG16-BN Encoder: extracts hierarchical features from input images\n",
        "class VGGEncoder(nn.Module):\n",
        "    def __init__(self,pretrained=True):\n",
        "        super(VGGEncoder, self).__init__()\n",
        "\n",
        "\n",
        "        # Load VGG16 with Batch Normalization (optionally pretrained on ImageNet)\n",
        "        vgg = models.vgg16_bn(pretrained=pretrained)\n",
        "\n",
        "        # Convert the feature layers to a list for manual slicing\n",
        "        features = list(vgg.features.children())\n",
        "\n",
        "        # Split VGG16 into 5 stages based on spatial downsampling:\n",
        "        # Each stage ends with a MaxPool (downsampling) layer\n",
        "\n",
        "        # Stage 1: conv1_1 to relu1_2 — output: (B, 64, H/2, W/2)\n",
        "        self.stage1 = nn.Sequential(*features[0:6])\n",
        "\n",
        "        # Stage 2: conv2_1 to relu2_2 — output: (B, 128, H/4, W/4)\n",
        "        self.stage2 = nn.Sequential(*features[6:13])\n",
        "\n",
        "        # Stage 3: conv3_1 to relu3_3 — output: (B, 256, H/8, W/8)\n",
        "        self.stage3 = nn.Sequential(*features[13:23])\n",
        "\n",
        "        # Stage 4: conv4_1 to relu4_3 — output: (B, 512, H/16, W/16)\n",
        "        self.stage4 = nn.Sequential(*features[23:33])\n",
        "\n",
        "        # Stage 5: conv5_1 to relu5_3 — output: (B, 512, H/32, W/32)\n",
        "        self.stage5 = nn.Sequential(*features[33:43])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through each stage sequentially, saving intermediate outputs\n",
        "        f1 = self.stage1(x)  # Low-level texture edges\n",
        "        f2 = self.stage2(f1) # Slightly more abstract edges\n",
        "        f3 = self.stage3(f2) # Mid-level patterns\n",
        "        f4 = self.stage4(f3) # High-level object parts\n",
        "        f5 = self.stage5(f4) # Deepest semantic features\n",
        "\n",
        "        # Return all feature maps for use in MSAB\n",
        "        return f1, f2, f3, f4, f5\n",
        "\n",
        "\n",
        "\n",
        "#MULTI SCALE ATTENTION BLOCK\n",
        "# Combines multi-scale convolutions and self-attention for feature refinement\n",
        "class MSAB(nn.Module):\n",
        "    def __init__(self, in_channels, out_size=(256, 256)):\n",
        "        super(MSAB, self).__init__()\n",
        "\n",
        "        # Multi-scale convolution branches\n",
        "        self.conv1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)  # Focus on channel-wise info\n",
        "        self.conv3x3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)  # Capture local spatial patterns\n",
        "\n",
        "        # Attention layers (query, key, value)\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.key_conv   = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "       # Learnable scaling factor to control attention contribution\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        # Final upsample output to a consistent size (e.g., for skip connections)\n",
        "        self.out_size = out_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape  # Batch size, Channels, Height, Width\n",
        "\n",
        "        f1 = self.conv1x1(x)  # Channel-focused features (fine details)\n",
        "        f2 = self.conv3x3(x)  # Spatial-focused features (edges, textures)\n",
        "\n",
        "       # Step 2: Self-attention mechanism\n",
        "        # Transform input into query, key, value\n",
        "        Q = self.query_conv(x).view(B, C, -1)              # (B, C, H*W)\n",
        "        K = self.key_conv(x).view(B, C, -1)                # (B, C, H*W)\n",
        "        V = self.value_conv(x).view(B, C, -1)              # (B, C, H*W)\n",
        "\n",
        "         # Compute attention scores: Qᵀ·K → (B, H*W, H*W)\n",
        "        attn = torch.bmm(Q.transpose(1, 2), K)\n",
        "        # Normalize the attention map using softmax\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # Weighted sum of values: attention-weighted features\n",
        "        Fmid = torch.bmm(V, attn.transpose(1, 2))  # (B, C, H*W)\n",
        "        Fmid = Fmid.view(B, C, H, W)               # Reshape back to (B, C, H, W)\n",
        "\n",
        "        # Step 3: Combine attention with multi-scale features\n",
        "        sFmid = self.scale * Fmid         # Scale the attention feature map\n",
        "        f1c = sFmid + f1                  # Fuse with 1x1 conv output\n",
        "        f2c = sFmid + f2                  # Fuse with 3x3 conv output\n",
        "\n",
        "        fused = f1c + f2c                 # Final feature fusion\n",
        "\n",
        "\n",
        "        # Step 4: Upsample to 256x256\n",
        "        Fmsab = F.interpolate(fused, size=self.out_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "        return Fmsab\n",
        "\n",
        "\n",
        "#LESON AWARE RELATION BLOCK\n",
        "class LARB(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels=64):\n",
        "        super(LARB, self).__init__()\n",
        "\n",
        "        self.lesion_types = ['MA', 'SE', 'HE', 'EX']\n",
        "\n",
        "        self.lesion_convs = nn.ModuleDict({\n",
        "            l: nn.Conv2d(in_channels, mid_channels, kernel_size=1)\n",
        "            for l in self.lesion_types\n",
        "        })\n",
        "\n",
        "        def lesion_attention():\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(mid_channels, mid_channels // 4, kernel_size=1),  #  channel dim reduction\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(mid_channels // 4, mid_channels, kernel_size=1)   # ↑ channel dim restored\n",
        "            )\n",
        "\n",
        "        self.attention_blocks = nn.ModuleDict({\n",
        "            l: lesion_attention() for l in self.lesion_types\n",
        "        })\n",
        "\n",
        "        self.reduction_convs = nn.ModuleDict({\n",
        "            l: nn.Conv2d(mid_channels, 1, kernel_size=1)\n",
        "            for l in self.lesion_types\n",
        "        })\n",
        "\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def apply_lah(self, FL, att_layer):\n",
        "        avg_pool = F.adaptive_avg_pool2d(FL, 1)\n",
        "        max_pool = F.adaptive_max_pool2d(FL, 1)\n",
        "        att = att_layer(avg_pool) + self.scale * att_layer(max_pool)\n",
        "        return FL * torch.sigmoid(att)\n",
        "\n",
        "    def forward(self, f3_out, f4_out, f5_out):\n",
        "        FM = torch.cat([f3_out, f4_out, f5_out], dim=1)  # [B, C_total, H, W]\n",
        "\n",
        "        outputs = {}\n",
        "        for l in self.lesion_types:\n",
        "            FL = self.lesion_convs[l](FM)\n",
        "            FL_att = self.apply_lah(FL, self.attention_blocks[l])\n",
        "            FR = self.reduction_convs[l](FL_att)\n",
        "            outputs[l] = FR\n",
        "\n",
        "        return outputs['MA'], outputs['SE'], outputs['HE'], outputs['EX']\n",
        "\n",
        "\n",
        "# =================**SPATIAL FUSION BLOCK**===============\n",
        "\n",
        "class SpatialFusionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=4, inter_channels=16):\n",
        "        super(SpatialFusionBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, inter_channels, kernel_size=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(inter_channels, 1, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, lesion_maps, image):\n",
        "        \"\"\"\n",
        "        lesion_maps: Tensor [B, 4, H, W]  -> MA, HE, EX, SE\n",
        "        image:       Tensor [B, 3, H, W]  -> original RGB image\n",
        "\n",
        "        Returns:\n",
        "        enhanced_image: [B, 3, H, W]\n",
        "        \"\"\"\n",
        "        x = self.conv1(lesion_maps)         # [B, 4, H, W]\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)                   # [B, 3, H, W]\n",
        "        attention_map = self.sigmoid(x)     # [B, 1, H, W]\n",
        "\n",
        "        # Resize attention_map to match image spatial size\n",
        "        if attention_map.shape[-2:] != image.shape[-2:]:\n",
        "            attention_map = F.interpolate(attention_map, size=image.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        enhanced_image = image * attention_map  # [B, 3, h, w]\n",
        "        return enhanced_image\n",
        "\n",
        "\n",
        "# ENHANCED SELF ATTENTION BLOCK\n",
        "class ESAB(nn.Module):\n",
        "    def __init__(self, shared_encoder, embed_dim=512):  # VGG16 f5 = 512 channels\n",
        "        super(ESAB, self).__init__()\n",
        "        self.encoder = shared_encoder\n",
        "\n",
        "        self.query_proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        self.key_proj   = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        self.value_proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "\n",
        "        self.ffd = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, image_sfb,x):\n",
        "        B, C, H, W = image_sfb.shape\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "\n",
        "        # Step 1: Get global feature from full image\n",
        "        global_feats = self.encoder(x)[-1]  # [B, 512, H/16, W/16 ] = [B, 512, 32, 32] for 512x512\n",
        "\n",
        "        # Step 2: Split input image into 4 patches (2x2)\n",
        "        patch_rows = torch.chunk(image_sfb, 2, dim=2)  # height\n",
        "        patches = [torch.chunk(r, 2, dim=3) for r in patch_rows]  # width\n",
        "        patches = [p for row in patches for p in row]  # flatten\n",
        "\n",
        "        # Step 3: Process each patch\n",
        "        local_outs = []\n",
        "        for patch in patches:\n",
        "            local_feat = self.encoder(patch)[-1]  # local f5\n",
        "\n",
        "            Q = self.query_proj(local_feat).flatten(2)       # [B, D, hw]\n",
        "            K = self.key_proj(global_feats).flatten(2)       # [B, D, HW]\n",
        "            V = self.value_proj(global_feats).flatten(2)     # [B, D, HW]\n",
        "\n",
        "            attn = torch.bmm(Q.transpose(1, 2), K) / (K.size(1) ** 0.5)  # [B, hw, HW]\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "            out = torch.bmm(V, attn.transpose(1, 2))         # [B, D, hw]\n",
        "            out = out.view_as(local_feat)\n",
        "            local_outs.append(self.ffd(out))\n",
        "\n",
        "        # Step 4: Re-stitch the 4 patches into a full feature map\n",
        "        top = torch.cat([local_outs[0], local_outs[1]], dim=3)\n",
        "        bottom = torch.cat([local_outs[2], local_outs[3]], dim=3)\n",
        "        stitched = torch.cat([top, bottom], dim=2)\n",
        "\n",
        "        # Step 5: Add residual global features\n",
        "        return stitched + global_feats  # [B, D, H/32, W/32]\n",
        "\n",
        "\n",
        "\n",
        "# ================**GRADING HEAD**==================\n",
        "\n",
        "class GradingHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, num_classes=5):\n",
        "        super(GradingHead, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)       # [B, C, 1, 1]\n",
        "        x = self.flatten(x)    # [B, C]\n",
        "        out = self.fc(x)       # [B, num_classes]\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========FULL MSGDANet CLASS WRAPPER=============\n",
        "\n",
        "# class MSGDANet(nn.Module):\n",
        "#     def __init__(self, num_classes=5, img_size=(256, 256)):\n",
        "#         super(MSGDANet, self).__init__()\n",
        "\n",
        "#         # Shared encoder\n",
        "#         self.encoder = VGGEncoder()\n",
        "\n",
        "#         # Multi-Scale Attention Blocks\n",
        "#         self.msab1 = MSAB(64, out_size=img_size)\n",
        "#         self.msab2 = MSAB(128, out_size=img_size)\n",
        "#         self.msab3 = MSAB(256, out_size=img_size)\n",
        "#         self.msab4 = MSAB(512, out_size=img_size)\n",
        "#         self.msab5 = MSAB(512, out_size=img_size)\n",
        "\n",
        "#         # Lesion-Aware Relation Block\n",
        "#         self.larb = LARB(in_channels=256 + 512 + 512, mid_channels=64)\n",
        "\n",
        "#         # Spatial Fusion Block\n",
        "#         self.sfb = SpatialFusionBlock(in_channels=4, inter_channels=16)\n",
        "\n",
        "#         # Enhanced Self-Attention Block\n",
        "#         self.esab = ESAB(shared_encoder=self.encoder, embed_dim=512)\n",
        "\n",
        "#         # DR Grading Head\n",
        "#         self.grading_head = GradingHead(in_channels=512, num_classes=num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         Input:\n",
        "#             x: RGB fundus image [B, 3, 512, 512]\n",
        "#         Output:\n",
        "#             lesion_maps: [B, 4, 512, 512] - MA, HE, EX, SE\n",
        "#             logits: [B, num_classes] - DR grades\n",
        "#         \"\"\"\n",
        "#         # Stage-wise feature extraction\n",
        "#         f1, f2, f3, f4, f5 = self.encoder(x)\n",
        "\n",
        "#         # Multi-scale attention per stage\n",
        "#         # fMSAB1 = self.msab1(f1)\n",
        "#         # fMSAB2 = self.msab2(f2)\n",
        "#         fMSAB3 = self.msab3(f3)\n",
        "#         fMSAB4 = self.msab4(f4)\n",
        "#         fMSAB5 = self.msab5(f5)\n",
        "\n",
        "#         # print(\"MSAB3 output:\", fMSAB3.min().item(), fMSAB3.max().item())\n",
        "#         # print(\"MSAB4 output:\", fMSAB4.min().item(), fMSAB4.max().item())\n",
        "#         # print(\"MSAB5 output:\", fMSAB5.min().item(), fMSAB5.max().item())\n",
        "\n",
        "#         fMSAB3 = F.interpolate(fMSAB3, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "#         fMSAB4 = F.interpolate(fMSAB4, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "#         fMSAB5 = F.interpolate(fMSAB5, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "#         # LARB: lesion-specific features\n",
        "#         FR_MA, FR_HE, FR_EX, FR_SE = self.larb(fMSAB3, fMSAB4, fMSAB5)\n",
        "\n",
        "#         # print(\"FR_ma:\", FR_ma.min().item(), FR_ma.max().item())\n",
        "#         # print(\"FR_he:\", FR_he.min().item(), FR_he.max().item())\n",
        "#         # print(\"FR_ex:\", FR_ex.min().item(), FR_ex.max().item())\n",
        "#         # print(\"FR_se:\", FR_se.min().item(), FR_se.max().item())\n",
        "\n",
        "#         # Stack lesion maps into single tensor [B, 4, H, W]\n",
        "#         lesion_maps = torch.cat([FR_MA, FR_HE, FR_EX, FR_SE], dim=1)\n",
        "#         lesion_maps = (lesion_maps - lesion_maps.mean()) / (lesion_maps.std() + 1e-6)\n",
        "#         lesion_maps = torch.sigmoid(lesion_maps)\n",
        "\n",
        "#         # SFB: spatial attention over RGB image\n",
        "#         isfb = self.sfb(lesion_maps, x)\n",
        "\n",
        "#         # ESAB: enhanced global-local fusion\n",
        "#         esab_out = self.esab(isfb)\n",
        "\n",
        "#         # Grading head\n",
        "#         logits = self.grading_head(esab_out)\n",
        "\n",
        "#         return lesion_maps, logits\n",
        "class MSGDANet(nn.Module):\n",
        "    def __init__(self, num_classes=5, img_size=(256, 256)):\n",
        "        super(MSGDANet, self).__init__()\n",
        "\n",
        "        # ─── Shared Encoder ─────────────────────────────────────────────────────\n",
        "        self.encoder = VGGEncoder()\n",
        "\n",
        "        # ─── Segmentation Branch ───────────────────────────────────────────────\n",
        "        self.segmentation_branch = nn.ModuleDict({\n",
        "            \"msab3\": MSAB(256, out_size=img_size),\n",
        "            \"msab4\": MSAB(512, out_size=img_size),\n",
        "            \"msab5\": MSAB(512, out_size=img_size),\n",
        "            \"larb\": LARB(in_channels=256 + 512 + 512, mid_channels=64)\n",
        "        })\n",
        "\n",
        "        # ─── Grading Branch ────────────────────────────────────────────────────\n",
        "        self.grading_branch = nn.ModuleDict({\n",
        "            \"sfb\": SpatialFusionBlock(in_channels=4, inter_channels=16),\n",
        "            \"esab\": ESAB(shared_encoder=self.encoder, embed_dim=512),\n",
        "            \"head\": GradingHead(in_channels=512, num_classes=num_classes)\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ─── Encoder Features ───\n",
        "        f1, f2, f3, f4, f5 = self.encoder(x)\n",
        "\n",
        "        # ─── MSAB ───\n",
        "        msab3 = self.segmentation_branch[\"msab3\"](f3)\n",
        "        msab4 = self.segmentation_branch[\"msab4\"](f4)\n",
        "        msab5 = self.segmentation_branch[\"msab5\"](f5)\n",
        "\n",
        "        # ─── Upsample to match output size ───\n",
        "        msab3 = F.interpolate(msab3, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "        msab4 = F.interpolate(msab4, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "        msab5 = F.interpolate(msab5, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # ─── LARB Lesion Maps ───\n",
        "        FR_MA, FR_HE, FR_EX, FR_SE = self.segmentation_branch[\"larb\"](msab3, msab4, msab5)\n",
        "        lesion_maps = torch.cat([FR_MA, FR_HE, FR_EX, FR_SE], dim=1)\n",
        "        lesion_maps = (lesion_maps - lesion_maps.mean()) / (lesion_maps.std() + 1e-6)\n",
        "        lesion_maps = torch.sigmoid(lesion_maps)\n",
        "\n",
        "        # ─── SFB + ESAB + Grading ───\n",
        "        isfb = self.grading_branch[\"sfb\"](lesion_maps, x)\n",
        "        esab_out = self.grading_branch[\"esab\"](isfb,x)\n",
        "        logits = self.grading_branch[\"head\"](esab_out)\n",
        "\n",
        "        return lesion_maps, logits\n",
        "\n",
        "# ==========**LOSS**=========\n",
        "class MSGDALoss(nn.Module):\n",
        "    def __init__(self, alpha, beta, lam, task=None):\n",
        "        super(MSGDALoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.lam = lam\n",
        "        self.task = task\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Lesion-specific weights (normalized)\n",
        "        lesion_weights = {'MA': 10, 'HE': 1, 'EX': 5, 'SE': 5}\n",
        "        total = sum(lesion_weights.values())\n",
        "        self.weights = {k: v / total for k, v in lesion_weights.items()}\n",
        "\n",
        "    def tversky_loss(self, pred, target):\n",
        "        smooth = 1e-6\n",
        "        pred = pred.reshape(-1)\n",
        "        target = target.reshape(-1)\n",
        "\n",
        "        tp = (pred * target).sum(dim=(0))\n",
        "        fp = ((1 - target) * pred).sum(dim=(0))\n",
        "        fn = (target * (1 - pred)).sum(dim=(0))\n",
        "\n",
        "        tversky = (tp + smooth) / (tp + self.alpha * fp + self.beta * fn + smooth)\n",
        "        return 1 - tversky\n",
        "\n",
        "    def forward(self, lesion_preds, lesion_targets, logits=None, labels=None):\n",
        "        device = lesion_preds.device if lesion_preds is not None else (\n",
        "             logits.device if logits is not None else 'cpu')\n",
        "        total_seg_loss = torch.tensor(0.0, device=lesion_preds.device if lesion_preds is not None else 'cpu')\n",
        "        total_cls_loss = torch.tensor(0.0, device=logits.device if logits is not None else 'cpu')\n",
        "\n",
        "        # Segmentation loss\n",
        "        if self.task in ['segmentation'] and lesion_preds is not None:\n",
        "            used= 0\n",
        "            skipped = 0\n",
        "            for i, lesion_type in enumerate(['MA', 'HE', 'EX', 'SE']):\n",
        "                pred = torch.clamp(lesion_preds[:, i, :, :], 1e-4, 1 - 1e-4)\n",
        "                target = lesion_targets[:, i, :, :]\n",
        "                assert pred.shape == target.shape, f\"Mismatch: pred {pred.shape}, target {target.shape}\"\n",
        "\n",
        "\n",
        "                # Skip dummy masks\n",
        "                if torch.all(target == 0):\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "                used += 1\n",
        "                # print(f\"Used lesions: {used}, Skipped: {skipped}\")\n",
        "\n",
        "\n",
        "                tversky = self.tversky_loss(pred, target)\n",
        "                bce = F.binary_cross_entropy(pred, target)\n",
        "                seg_loss = self.weights[lesion_type] * (tversky + self.lam * bce)\n",
        "                total_seg_loss += seg_loss\n",
        "\n",
        "        # Grading loss\n",
        "        if self.task in ['grading'] and logits is not None and labels is not None:\n",
        "            valid_mask = labels != -1\n",
        "            if valid_mask.any():\n",
        "                total_cls_loss = self.ce_loss(logits[valid_mask], labels[valid_mask])\n",
        "\n",
        "        total_loss = total_seg_loss + total_cls_loss\n",
        "        return total_loss, total_seg_loss, total_cls_loss\n",
        "\n",
        "\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.grading_preds = []\n",
        "        self.grading_labels = []\n",
        "\n",
        "        self.segmentation_preds = []\n",
        "        self.segmentation_targets = []\n",
        "\n",
        "    def update_grading(self, preds, labels):\n",
        "        self.grading_preds.append(preds.detach().cpu())\n",
        "        self.grading_labels.append(labels.detach().cpu())\n",
        "\n",
        "    def update_segmentation(self, preds, targets):\n",
        "        self.segmentation_preds.append(preds.detach().cpu())\n",
        "        self.segmentation_targets.append(targets.detach().cpu())\n",
        "\n",
        "    def compute_grading_metrics(self):\n",
        "        if not self.grading_preds:\n",
        "            return 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        preds = torch.cat(self.grading_preds).numpy()\n",
        "        labels = torch.cat(self.grading_labels).numpy()\n",
        "\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
        "        kappa = cohen_kappa_score(labels, preds)\n",
        "\n",
        "        return acc, precision, recall, kappa\n",
        "\n",
        "    def compute_segmentation_metrics(self):\n",
        "        if not self.segmentation_preds:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "        dice_scores, iou_scores, aupr_scores = [], [], []\n",
        "\n",
        "        for pred, target in zip(self.segmentation_preds, self.segmentation_targets):\n",
        "            B, C, H, W = pred.shape  # C = number of lesions\n",
        "            pred_flat = pred.view(B * C, -1)\n",
        "            target_flat = target.view(B * C, -1)\n",
        "\n",
        "            for i in range(B * C):\n",
        "                p = pred_flat[i].float()\n",
        "                t = target_flat[i].float()\n",
        "                p = (p > 0.5).float()\n",
        "                t = (t > 0.5).float()\n",
        "\n",
        "                TP = (p * t).sum().item()\n",
        "                FP = (p * (1 - t)).sum().item()\n",
        "                FN = ((1 - p) * t).sum().item()\n",
        "\n",
        "                dice = (2 * TP) / (2 * TP + FP + FN + 1e-6)\n",
        "                iou = TP / (TP + FP + FN + 1e-6)\n",
        "\n",
        "                # AUPR\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "                    try:\n",
        "                        precision, recall, _ = precision_recall_curve(t.numpy(), p.numpy())\n",
        "                        aupr = auc(recall, precision)\n",
        "                    except:\n",
        "                        aupr = 0.0\n",
        "\n",
        "                dice_scores.append(dice)\n",
        "                iou_scores.append(iou)\n",
        "                aupr_scores.append(aupr)\n",
        "\n",
        "        mean_dice = sum(dice_scores) / len(dice_scores)\n",
        "        mean_iou = sum(iou_scores) / len(iou_scores)\n",
        "        mean_aupr = sum(aupr_scores) / len(aupr_scores)\n",
        "\n",
        "        return mean_dice, mean_iou, mean_aupr\n",
        "\n",
        "    def summarize(self, train_loss, val_loss, train_acc, task):\n",
        "        val_acc, prec, rec, kappa = self.compute_grading_metrics()\n",
        "        dice, iou, aupr = self.compute_segmentation_metrics()\n",
        "\n",
        "        # Prepare base headers and values\n",
        "        headers = [\"Train Loss\", \"Val Loss\"]\n",
        "        values = [f\"{train_loss:.2f}\", f\"{val_loss:.2f}\"]\n",
        "\n",
        "        # Show task-specific training metric\n",
        "        if task == \"grading\":\n",
        "            headers.append(\"Train Acc\")\n",
        "            values.append(f\"{train_acc:.2f}\")\n",
        "        elif task == \"segmentation\":\n",
        "            headers.append(\"Train Dice\")\n",
        "            values.append(f\"{train_acc:.4f}\")  # here train_acc is actually Dice for segmentation\n",
        "\n",
        "        # Append validation metrics\n",
        "        if task == \"grading\":\n",
        "            headers += [\"Val Acc\", \"Precision\", \"Recall\", \"Kappa\"]\n",
        "            values += [f\"{val_acc:.2f}\", f\"{prec:.2f}\", f\"{rec:.2f}\", f\"{kappa:.2f}\"]\n",
        "        elif task == \"segmentation\":\n",
        "            headers += [\"Dice\", \"IoU\", \"AUPR\"]\n",
        "            values += [f\"{dice:.4f}\", f\"{iou:.4f}\", f\"{aupr:.4f}\"]\n",
        "\n",
        "        print(\"\\n\" + tabulate([values], headers=headers, tablefmt=\"pretty\"))\n",
        "\n",
        "        # Return the metric to monitor\n",
        "        if task == \"grading\":\n",
        "            return val_acc\n",
        "        elif task == \"segmentation\":\n",
        "            return dice\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiTaskDRDataset(Dataset):\n",
        "    def __init__(self, root_dir, dataset_name, split='train', transform=None, image_size=(256, 256), task=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.image_size = image_size\n",
        "        self.task = task\n",
        "        self.lesion_names = ['MA', 'HE', 'EX', 'SE']\n",
        "        self.samples = self._load_metadata()\n",
        "\n",
        "    def _load_metadata(self):\n",
        "        samples = []\n",
        "\n",
        "        if self.dataset_name == 'DDR':\n",
        "            if self.task == 'grading':\n",
        "                grading_csv = os.path.join(self.root_dir, 'DDR/grading', f'{self.split}.csv')\n",
        "                if os.path.exists(grading_csv):\n",
        "                    df = pd.read_csv(grading_csv)\n",
        "                    for _, row in df.iterrows():\n",
        "                        if row['Retinopathy grade'] == 5:\n",
        "                            continue  # skip optic disc\n",
        "                        img_name = row['Image Name']\n",
        "                        img_path = os.path.join(self.root_dir, f'DDR/grading/{self.split}', img_name)\n",
        "                        samples.append({\n",
        "                            'img_name': img_name,\n",
        "                            'img_path': img_path,\n",
        "                            'label': int(row['Retinopathy grade']),\n",
        "                            'masks': None,\n",
        "                        })\n",
        "\n",
        "            elif self.task == 'segmentation':\n",
        "                seg_img_dir = os.path.join(self.root_dir, f'DDR/segmentation/{self.split}/image')\n",
        "                for img_name in os.listdir(seg_img_dir):\n",
        "                    img_path = os.path.join(seg_img_dir, img_name)\n",
        "                    masks = {\n",
        "                        lesion: os.path.join(\n",
        "                            self.root_dir,\n",
        "                            f'DDR/segmentation/{self.split}/groundtruth/{lesion}',\n",
        "                            img_name.replace('.jpg', '.tif')\n",
        "                        )\n",
        "                        for lesion in self.lesion_names\n",
        "                    }\n",
        "                    samples.append({\n",
        "                        'img_name': img_name,\n",
        "                        'img_path': img_path,\n",
        "                        'label': -1,\n",
        "                        'masks': masks\n",
        "                    })\n",
        "\n",
        "        elif self.dataset_name == 'IDRID':\n",
        "            if self.task == 'grading':\n",
        "                if self.split == 'train':\n",
        "                    grading_csv = os.path.join(self.root_dir, 'IDRID/Grading/Groundtruths/IDRiD_Disease_Grading_Training_Labels.csv')\n",
        "                    grading_dir = os.path.join(self.root_dir, 'IDRID/Grading/Fundus Images/Training Set')\n",
        "                else:\n",
        "                    grading_csv = os.path.join(self.root_dir, 'IDRID/Grading/Groundtruths/IDRiD_Disease_Grading_Testing_Labels.csv')\n",
        "                    grading_dir = os.path.join(self.root_dir, 'IDRID/Grading/Fundus Images/Testing Set')\n",
        "\n",
        "                df = pd.read_csv(grading_csv)\n",
        "                for _, row in df.iterrows():\n",
        "                    if row['Retinopathy grade'] == 5:\n",
        "                            continue  # skip optic disc\n",
        "                    img_name = row['Image name']\n",
        "                    img_path = os.path.join(grading_dir, f\"{img_name}.jpg\")\n",
        "                    samples.append({\n",
        "                        'img_name': img_name,\n",
        "                        'img_path': img_path,\n",
        "                        'label': int(row['Retinopathy grade']),\n",
        "                        'masks': None,\n",
        "                    })\n",
        "\n",
        "            elif self.task == 'segmentation':\n",
        "                seg_dir = os.path.join(self.root_dir, f'IDRID/Segmentation/Fundus Images/{self.split.capitalize()} Set')\n",
        "                mask_dir = os.path.join(self.root_dir, f'IDRID/Segmentation/Masks/{self.split.capitalize()} Set')\n",
        "                for img_name in os.listdir(seg_dir):\n",
        "                    name = img_name.replace('.jpg', '')\n",
        "                    img_path = os.path.join(seg_dir, img_name)\n",
        "                    masks = {\n",
        "                        lesion: os.path.join(mask_dir, lesion, f\"{name}_{lesion}.tif\") for lesion in self.lesion_names\n",
        "                    }\n",
        "                    samples.append({\n",
        "                        'img_name': name,\n",
        "                        'img_path': img_path,\n",
        "                        'label': -1,\n",
        "                        'masks': masks,\n",
        "                    })\n",
        "\n",
        "        elif self.dataset_name == 'APTOS':\n",
        "            csv_path = os.path.join(self.root_dir, 'APTOS', f'{self.split}.csv')\n",
        "            img_dir = os.path.join(self.root_dir, 'APTOS', f'{self.split}_images')\n",
        "            df = pd.read_csv(csv_path)\n",
        "            for _, row in df.iterrows():\n",
        "                img_id = row['id_code']\n",
        "                img_path = os.path.join(img_dir, f\"{img_id}.png\")\n",
        "                samples.append({\n",
        "                    'img_name': img_id,\n",
        "                    'img_path': img_path,\n",
        "                    'label': int(row['diagnosis']),\n",
        "                    'masks': None,\n",
        "                })\n",
        "\n",
        "\n",
        "        # Filter samples depending on task\n",
        "        if self.task == 'grading':\n",
        "            samples = [s for s in samples if s['label'] != -1]\n",
        "        elif self.task == 'segmentation':\n",
        "            samples = [s for s in samples if s['masks'] is not None]\n",
        "\n",
        "        # Print debug info\n",
        "        num_grading = sum(1 for s in samples if s['label'] != -1)\n",
        "        num_segmentation = sum(1 for s in samples if s['masks'] is not None)\n",
        "        print(f\"Loaded {len(samples)} samples from {self.dataset_name} {self.split} for {self.task} task.\")\n",
        "        print(f\" - Grading samples: {num_grading}\")\n",
        "        print(f\" - Segmentation samples: {num_segmentation}\")\n",
        "        return samples\n",
        "\n",
        "    def _load_mask(self, path):\n",
        "        if path and os.path.exists(path):\n",
        "            return np.array(Image.open(path).convert('L').resize(self.image_size))\n",
        "        else:\n",
        "            return np.zeros(self.image_size, dtype=np.uint8)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(sample['img_path']).convert('RGB')\n",
        "            image = np.array(image)\n",
        "            image = cv2.resize(image, self.image_size)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {sample['img_path']}: {e}\")\n",
        "            return self.__getitem__((idx + 1) % len(self.samples))\n",
        "\n",
        "        if self.task == 'segmentation':\n",
        "            try:\n",
        "                mask_channels = [self._load_mask(sample['masks'].get(lesion)) for lesion in self.lesion_names]\n",
        "                multi_mask = np.stack(mask_channels, axis=-1).astype(np.float32) / 255.0\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading masks for {sample['img_name']}: {e}\")\n",
        "                multi_mask = np.zeros((*self.image_size, len(self.lesion_names)), dtype=np.float32)\n",
        "        else:\n",
        "            multi_mask = np.zeros((*self.image_size, len(self.lesion_names)), dtype=np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=multi_mask)\n",
        "            image = augmented['image']\n",
        "            multi_mask = augmented['mask']\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image)\n",
        "            multi_mask = torch.from_numpy(multi_mask).permute(2, 0, 1).float()\n",
        "\n",
        "        label = sample['label'] if sample['label'] is not None else -1\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': torch.tensor(label, dtype=torch.long),\n",
        "            'masks': (multi_mask > 0.5).float(),\n",
        "        }\n",
        "\n",
        "\n",
        "# ===========**TRANSFORMS**==========\n",
        "train_transforms = A.Compose([\n",
        "  A.Resize(256, 256),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  A.VerticalFlip(p=0.5),\n",
        "  A.RandomRotate90(p=1.0),\n",
        "  A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=30, p=0.5),\n",
        "  A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),              # Enhances local contrast using Adaptive Histogram Equalization\n",
        "  A.Normalize(mean=(0.5), std=(0.5),max_pixel_value=255.0),\n",
        "  ToTensorV2()])\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "  A.Resize(256, 256),\n",
        "  A.Normalize(mean=(0.5), std=(0.5),max_pixel_value=255.0),\n",
        "  ToTensorV2()])\n",
        "\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class RunningAverage:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.sum = defaultdict(float)\n",
        "        self.count = 0\n",
        "    def update(self, **kwargs):\n",
        "        self.count += 1\n",
        "        for k, v in kwargs.items():\n",
        "            self.sum[k] += v\n",
        "    def avg(self, k):\n",
        "        return self.sum[k] / self.count if self.count > 0 else 0.0\n",
        "\n",
        "\n",
        "\n",
        "# **TRAINING** **SCRIPT**\n",
        "\n",
        "# ===Config ===\n",
        "#Setting working directory\n",
        "drive.mount('/content/drive')\n",
        "project_root = '/content/drive/MyDrive/vgg'\n",
        "#project_root = os.path.expanduser(\"~/MSGDANet\")\n",
        "sys.path.append(project_root)\n",
        "ROOT_DIR = os.path.join(project_root, \"data\")\n",
        "\n",
        "#Configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATASET_NAME = 'DDR' #'IDRID' or 'APTOS'\n",
        "BATCH_SIZE = 2\n",
        "NUM_CLASSES = 5\n",
        "EARLY_STOP_PATIENCE = 20\n",
        "STAGE = 'stage1'  # or 'stage2'\n",
        "if STAGE == 'stage1':\n",
        "    TASK = 'segmentation'\n",
        "elif STAGE == 'stage2':\n",
        "    TASK = 'grading'\n",
        "\n",
        "\n",
        "# Dataset-specific hyperparameters\n",
        "CONFIG = {\n",
        "    'DDR':   {'epochs': 380, 'lr': 1e-4, 'weight_decay': 2e-4},\n",
        "    'IDRID': {'epochs': 500, 'lr': 2e-3, 'weight_decay': 5e-4},\n",
        "    'APTOS': {'epochs': 330, 'lr': 1e-3, 'weight_decay': 1e-4},\n",
        "}\n",
        "params = CONFIG[DATASET_NAME]\n",
        "NUM_EPOCHS = params['epochs']\n",
        "LR = params['lr']\n",
        "WEIGHT_DECAY = params['weight_decay']\n",
        "\n",
        "# ============DATA  LOADERS===========\n",
        "train_dataset = MultiTaskDRDataset(dataset_name=DATASET_NAME, root_dir=ROOT_DIR, split='train',task=TASK)\n",
        "val_dataset = MultiTaskDRDataset(dataset_name=DATASET_NAME, root_dir=ROOT_DIR, split='val',task= TASK)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "#Checkpoint path setup\n",
        "checkpoint_dir = os.path.join(project_root, \"checkpoints\")\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "#Visualizations path setup\n",
        "visualization_dir = os.path.join(project_root, \"visualizations\")\n",
        "os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def visualize_predictions(images, masks, preds, num_samples=3, save_path=None, show_inline=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    images = images.cpu()\n",
        "    masks = masks.cpu()\n",
        "    preds = preds.cpu()\n",
        "\n",
        "    for i in range(min(num_samples, images.size(0))):\n",
        "        fig, axs = plt.subplots(3, masks.shape[1], figsize=(4 * masks.shape[1], 10))\n",
        "\n",
        "        for c in range(masks.shape[1]):\n",
        "            axs[0, c].imshow(images[i].permute(1, 2, 0))\n",
        "            axs[0, c].set_title(\"Image\")\n",
        "            axs[0, c].axis(\"off\")\n",
        "\n",
        "            axs[1, c].imshow(masks[i, c], cmap='gray')\n",
        "            axs[1, c].set_title(f\"GT Mask (Lesion {c})\")\n",
        "            axs[1, c].axis(\"off\")\n",
        "\n",
        "            axs[2, c].imshow(preds[i, c], cmap='gray')\n",
        "            axs[2, c].set_title(f\"Pred Mask (Lesion {c})\")\n",
        "            axs[2, c].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            out_path = f\"{save_path}_sample_{i}.png\"\n",
        "            plt.savefig(out_path)\n",
        "            print(f\"Saved visualization: {out_path}\")\n",
        "\n",
        "        # if show_inline:\n",
        "        #     plt.show()\n",
        "        # else:\n",
        "        #     plt.close()\n",
        "\n",
        "# === Unified Training Loop for Both Stages ===\n",
        "for STAGE in ['segmentation', 'grading']:\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"Starting Stage: {STAGE.upper()}\")\n",
        "    print(f\"============================\\n\")\n",
        "\n",
        "    TASK = STAGE\n",
        "    model = MSGDANet(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    loss_fn = MSGDALoss(alpha=0.0, beta=0.0, lam=1.0, task=TASK).to(DEVICE)\n",
        "\n",
        "    # Load best segmentation checkpoint for grading stage\n",
        "    if TASK == 'grading':\n",
        "        seg_ckpt_path = os.path.join(checkpoint_dir, f'msgdanet_best_{DATASET_NAME.lower()}_segmentation.pth')\n",
        "        if os.path.exists(seg_ckpt_path):\n",
        "            model.load_state_dict(torch.load(seg_ckpt_path, map_location=DEVICE))\n",
        "            print(f\"Loaded segmentation checkpoint for grading stage: {seg_ckpt_path}\")\n",
        "        else:\n",
        "            print(f\"Segmentation checkpoint not found at {seg_ckpt_path}. Training grading stage from scratch.\")\n",
        "\n",
        "\n",
        "        # Freeze encoder, MSABs, LARB\n",
        "        for param in model.encoder.parameters(): param.requires_grad = False\n",
        "        for msab in [model.segmentation_branch[\"msab3\"], model.segmentation_branch[\"msab4\"], model.segmentation_branch[\"msab5\"]]:\n",
        "            for param in msab.parameters(): param.requires_grad = False\n",
        "        for param in model.segmentation_branch[\"larb\"].parameters(): param.requires_grad = False\n",
        "\n",
        "    optimizer = Adan(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                     lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999, 0.999))\n",
        "    scheduler = PolynomialLR(optimizer, total_iters=NUM_EPOCHS, power=0.9)\n",
        "\n",
        "\n",
        "    # Checkpoint paths\n",
        "    ckpt_file = os.path.join(checkpoint_dir, f'checkpoint_{DATASET_NAME.lower()}_{TASK}.pth')\n",
        "    best_model_path = os.path.join(checkpoint_dir, f'msgdanet_best_{DATASET_NAME.lower()}_{TASK}.pth')\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_score = -float(\"inf\")\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    if os.path.exists(ckpt_file):\n",
        "        checkpoint = torch.load(ckpt_file, map_location=DEVICE)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
        "        best_score = checkpoint['best_score']\n",
        "        early_stop_counter = checkpoint['early_stop_counter']\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resumed from checkpoint: {ckpt_file}\")\n",
        "\n",
        "    # ========== Train Loop ==========\n",
        "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "        model.train()\n",
        "        running = RunningAverage()\n",
        "\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"[{TASK.upper()}] Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            masks = batch['masks'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "\n",
        "            if epoch == 0 and pbar.n < 3:\n",
        "               lesion_mask_stats = (masks.sum(dim=(2, 3)) > 0).float().mean(dim=0)\n",
        "               print(\"Lesion presence ratio:\", lesion_mask_stats.tolist())\n",
        "\n",
        "            if epoch == 0 and pbar.n < 4 and TASK == 'segmentation': # Only print for segmentation stage\n",
        "                with torch.no_grad(): # avoid calculating gradients for this print statement\n",
        "                    seg_preds, _ = model(images) # get predictions without affecting gradient calculation\n",
        "                    # print(\"Seg preds stats:\", seg_preds.min().item(), seg_preds.max().item())\n",
        "                    # print(\"Masks stats:\", masks.min().item(), masks.max().item())\n",
        "                    # print(\"Seg preds shape:\", seg_preds.shape)\n",
        "                    # print(\"Masks shape:\", masks.shape)\n",
        "                    # print(\"Pred mean:\", seg_preds.mean().item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            seg_preds, grade_logits = model(images)\n",
        "\n",
        "            if TASK == 'grading':\n",
        "                seg_preds = seg_preds.detach()\n",
        "            if TASK == 'segmentation':\n",
        "                grade_logits = grade_logits.detach()\n",
        "\n",
        "            total_loss, seg_loss, cls_loss = loss_fn(seg_preds, masks, grade_logits, labels)\n",
        "            #print(f\"Epoch {epoch} | Seg Loss: {seg_loss.item():.4f} | Cls Loss: {cls_loss.item():.4f}\")\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Metrics\n",
        "            with torch.no_grad():\n",
        "                batch_acc = 0.0\n",
        "                batch_dice = 0.0\n",
        "                valid_channels =0.0\n",
        "                if TASK == 'grading':\n",
        "                    preds = torch.argmax(grade_logits, dim=1)\n",
        "                    valid = (labels != -1)\n",
        "                    if valid.any():\n",
        "                        batch_acc = (preds[valid] == labels[valid]).float().mean().item()\n",
        "                if TASK == 'segmentation':\n",
        "                    probs = torch.sigmoid(seg_preds)\n",
        "                    pred_bin = (probs > 0.5).float()\n",
        "\n",
        "                    for i in range(4):  # MA, SE, HE, EX\n",
        "                        p = pred_bin[:, i].float()\n",
        "                        t = masks[:, i].float()\n",
        "\n",
        "                        if t.sum() > 0:\n",
        "                            inter = (p * t).sum((1, 2))\n",
        "                            union = p.sum((1, 2)) + t.sum((1, 2))\n",
        "                            dice = (2 * inter / (union + 1e-6)).mean().item()\n",
        "                            batch_dice += dice\n",
        "                            valid_channels += 1\n",
        "\n",
        "                    if valid_channels > 0:\n",
        "                        batch_dice /= valid_channels\n",
        "            # Track\n",
        "            update_kwargs = {'loss': total_loss.item()}\n",
        "            postfix = {'L': f\"{running.avg('loss'):.2f}\"}\n",
        "            if TASK == 'grading':\n",
        "                update_kwargs['acc'] = batch_acc\n",
        "                postfix['A'] = f\"{running.avg('acc'):.2f}\"\n",
        "            if TASK == 'segmentation':\n",
        "                update_kwargs['dice'] = batch_dice\n",
        "                postfix['D'] = f\"{running.avg('dice'):.4f}\" # Changed to 4 decimal places for dice\n",
        "            running.update(**update_kwargs)\n",
        "            pbar.set_postfix(postfix)\n",
        "\n",
        "        avg_train_loss = running.avg('loss')\n",
        "        avg_train_acc = running.avg('acc') if TASK == 'grading' else 0.0 # Only get acc for grading\n",
        "        avg_train_dice = running.avg('dice') if TASK == 'segmentation' else 0.0 # Only get dice for segmentation\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        # ===== Validation =====\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_tracker = MetricsTracker()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Validation\")):\n",
        "                images = batch['image'].to(DEVICE)\n",
        "                masks = batch['masks'].to(DEVICE)\n",
        "                labels = batch['label'].to(DEVICE)\n",
        "\n",
        "                seg_preds, logits = model(images)\n",
        "\n",
        "                if TASK == 'grading':\n",
        "                    seg_preds = seg_preds.detach()\n",
        "                if TASK == 'segmentation':\n",
        "                    logits = logits.detach()\n",
        "\n",
        "                loss, _, _ = loss_fn(seg_preds, masks, logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                if TASK == 'grading':\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    val_tracker.update_grading(preds, labels)\n",
        "\n",
        "                if TASK == 'segmentation':\n",
        "                    bin_preds = (torch.sigmoid(seg_preds) > 0.5).float()\n",
        "                    val_tracker.update_segmentation(bin_preds, masks)\n",
        "\n",
        "                    if batch_idx == 0:\n",
        "                      vis_base = os.path.join(visualization_dir, f\"{TASK}_epoch_{epoch+1:03d}\")\n",
        "                      visualize_predictions(images, masks, bin_preds, num_samples=1, save_path=vis_base, show_inline=True)\n",
        "\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        score = val_tracker.summarize(\n",
        "            train_loss=avg_train_loss,\n",
        "            val_loss=avg_val_loss,\n",
        "            train_acc=avg_train_acc if TASK == 'grading' else avg_train_dice,\n",
        "            task=TASK\n",
        "        )\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state': model.state_dict(),\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'scheduler_state': scheduler.state_dict(),\n",
        "            'best_score': best_score,\n",
        "            'early_stop_counter': early_stop_counter\n",
        "        }, ckpt_file)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            early_stop_counter = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(\"✅ Best model saved.\")\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= EARLY_STOP_PATIENCE:\n",
        "                print(\"⏹️ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "print(\"\\n🏁 Both stages complete! Best models saved.\\n\")"
      ],
      "metadata": {
        "id": "gluE-CB9ydNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jh1Qrl7qy0zC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}